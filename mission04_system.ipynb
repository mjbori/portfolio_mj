{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥í”„ë¡œì íŠ¸ : ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ê°ì„±ë¶„ì„ ë„ì „í•˜ê¸°\n",
    "\n",
    "- í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ê°ì •ë¶„ì„\n",
    "\n",
    "- ë°ì´í„°ì…‹: ë„¤ì´ë²„ ì˜í™”ì˜ ëŒ“ê¸€ì„ ëª¨ì•„ êµ¬ì„±ëœ Naver sentiment movie corpus\n",
    "\n",
    "> wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt  <br/>\n",
    " wget https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt    <br/>\n",
    " mv ratings_*.txt ~/aiffel/sentiment_classification\n",
    " \n",
    " \n",
    " ## 1) ë°ì´í„° ì¤€ë¹„ì™€ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myungjin-kim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  \n",
      "/home/myungjin-kim/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                ì•„ ë”ë¹™.. ì§„ì§œ ì§œì¦ë‚˜ë„¤ìš” ëª©ì†Œë¦¬      0\n",
       "1   3819312                  í ...í¬ìŠ¤í„°ë³´ê³  ì´ˆë”©ì˜í™”ì¤„....ì˜¤ë²„ì—°ê¸°ì¡°ì°¨ ê°€ë³ì§€ ì•Šêµ¬ë‚˜      1\n",
       "2  10265843                                  ë„ˆë¬´ì¬ë°“ì—ˆë‹¤ê·¸ë˜ì„œë³´ëŠ”ê²ƒì„ì¶”ì²œí•œë‹¤      0\n",
       "3   9045019                      êµë„ì†Œ ì´ì•¼ê¸°êµ¬ë¨¼ ..ì†”ì§íˆ ì¬ë¯¸ëŠ” ì—†ë‹¤..í‰ì  ì¡°ì •      0\n",
       "4   6483659  ì‚¬ì´ëª¬í˜ê·¸ì˜ ìµì‚´ìŠ¤ëŸ° ì—°ê¸°ê°€ ë‹ë³´ì˜€ë˜ ì˜í™”!ìŠ¤íŒŒì´ë”ë§¨ì—ì„œ ëŠ™ì–´ë³´ì´ê¸°ë§Œ í–ˆë˜ ì»¤ìŠ¤í‹´ ...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# ë°ì´í„° ì‚´í´ë³´ê¸° \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) ë°ì´í„°ë¡œë” êµ¬ì„±\n",
    "\n",
    "- IMDB ë°ì´í„°ì…‹: í…ìŠ¤íŠ¸ë¥¼ ê°€ê³µí•˜ì—¬ imdb.data_loader() ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ë©´ ìˆ«ì ì¸ë±ìŠ¤ë¡œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ì™€ word_to_index ë”•ì…”ë„ˆë¦¬ê¹Œì§€ ì œê³µ. \n",
    "\n",
    "\n",
    "- nsmc ë°ì´í„°ì…‹: ì „í˜€ ê°€ê³µë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŒ. ì´ê²ƒì„ ì½ì–´ì„œ imdb.data_loader()ì™€ ë™ì¼í•˜ê²Œ ë™ì‘í•˜ëŠ” ìì‹ ë§Œì˜ data_loaderë¥¼ ë§Œë“¤ì–´ ë³´ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘. \n",
    "\n",
    "\n",
    "- data_loader ì•ˆì—ì„œ ìˆ˜í–‰ ì¡°ê±´\n",
    "\n",
    "   - ë°ì´í„°ì˜ ì¤‘ë³µ ì œê±°\n",
    "   - NaN ê²°ì¸¡ì¹˜ ì œê±°\n",
    "   - í•œêµ­ì–´ í† í¬ë‚˜ì´ì €ë¡œ í† í°í™”\n",
    "   - ë¶ˆìš©ì–´(Stopwords) ì œê±°\n",
    "   - ì‚¬ì „word_to_index êµ¬ì„±\n",
    "   - í…ìŠ¤íŠ¸ ìŠ¤íŠ¸ë§ì„ ì‚¬ì „ ì¸ë±ìŠ¤ ìŠ¤íŠ¸ë§ìœ¼ë¡œ ë³€í™˜\n",
    "   - X_train, y_train, X_test, y_test, word_to_index ë¦¬í„´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "\n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # í† í°í™”\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # í† í°í™”\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # ë¶ˆìš©ì–´ ì œê±°\n",
    "        X_test.append(temp_X)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000-4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "\n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "\n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "\n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index\n",
    "\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì¥ 1ê°œë¥¼ í™œìš©í•  ë”•ì…”ë„ˆë¦¬ì™€ í•¨ê»˜ ì£¼ë©´, ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë²¡í„°ë¡œ ë³€í™˜í•´ ì£¼ëŠ” í•¨ìˆ˜\n",
    "# ë‹¨, ëª¨ë“  ë¬¸ì¥ì€ <BOS>ë¡œ ì‹œì‘í•˜ëŠ” ê²ƒ í•˜ì.\n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# ì—¬ëŸ¬ ê°œì˜ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ë¥¼ í•œêº¼ë²ˆì— ë‹¨ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë²¡í„°ë¡œ encodeí•´ ì£¼ëŠ” í•¨ìˆ˜\n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]ë¥¼ í†µí•´ <BOS>ë¥¼ ì œì™¸\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ ìˆ«ì ë²¡í„°ë¡œ encodeëœ ë¬¸ì¥ì„ í•œêº¼ë²ˆì— ì›ë˜ëŒ€ë¡œ decodeí•˜ëŠ” í•¨ìˆ˜\n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) ëª¨ë¸êµ¬ì„±ì„ ìœ„í•œ ë°ì´í„° ë¶„ì„ ë° ê°€ê³µ\n",
    "ë°ì´í„°ì…‹ ë‚´ ë¬¸ì¥ ê¸¸ì´ ë¶„í¬\n",
    "ì ì ˆí•œ ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ ì§€ì •\n",
    "keras.preprocessing.sequence.pad_sequences ì„ í™œìš©í•œ íŒ¨ë”© ì¶”ê°€\n",
    "\n",
    "## 4) ëª¨ë¸êµ¬ì„± ë° validation set êµ¬ì„±\n",
    "\n",
    "ëª¨ë¸ì€ 3ê°€ì§€ ì´ìƒ ë‹¤ì–‘í•˜ê²Œ êµ¬ì„±í•˜ì—¬ ì‹¤í—˜í•´ ë³´ì„¸ìš”.\n",
    "\n",
    "## 5) ëª¨ë¸ í›ˆë ¨ ê°œì‹œ\n",
    "\n",
    "## 6) Loss, Accuracy ê·¸ë˜í”„ ì‹œê°í™”\n",
    "\n",
    "## 7) í•™ìŠµëœ Embedding ë ˆì´ì–´ ë¶„ì„\n",
    "\n",
    "## 8) í•œêµ­ì–´ Word2Vec ì„ë² ë”© í™œìš©í•˜ì—¬ ì„±ëŠ¥ê°œì„ \n",
    "í•œêµ­ì–´ Word2Vecì€ ë‹¤ìŒ ê²½ë¡œì—ì„œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "https://github.com/Kyubyong/wordvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ğŸ˜­ ğŸ˜­ ë„ˆë¬´ ë²…ì°¨ì„œ... ì´ë²ˆì£¼ëŠ” ì¶”í›„ì— ë” í•´ë³¼ê²Œìš” ã… ã… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ ë£¨ë¸Œë¦­ ]\n",
    "\n",
    "\n",
    "1. ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ Text Classification íƒœìŠ¤í¬ë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ë‹¤.\n",
    "\n",
    "3ê°€ì§€ ì´ìƒì˜ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì‹œë„ë¨\n",
    "\n",
    "2. gensimì„ í™œìš©í•˜ì—¬ ìì²´í•™ìŠµëœ í˜¹ì€ ì‚¬ì „í•™ìŠµëœ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ë¶„ì„í•˜ì˜€ë‹¤.\n",
    "\n",
    "gensimì˜ ìœ ì‚¬ë‹¨ì–´ ì°¾ê¸°ë¥¼ í™œìš©í•˜ì—¬ ìì²´í•™ìŠµí•œ ì„ë² ë”©ê³¼ ì‚¬ì „í•™ìŠµ ì„ë² ë”©ì„ ì ì ˆíˆ ë¶„ì„í•¨\n",
    "\n",
    "3. í•œêµ­ì–´ Word2Vecì„ í™œìš©í•˜ì—¬ ê°€ì‹œì ì¸ ì„±ëŠ¥í–¥ìƒì„ ë‹¬ì„±í–ˆë‹¤.\n",
    "\n",
    "ë„¤ì´ë²„ ì˜í™”ë¦¬ë·° ë°ì´í„° ê°ì„±ë¶„ì„ ì •í™•ë„ë¥¼ 85% ì´ìƒ ë‹¬ì„±í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
