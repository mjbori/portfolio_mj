{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ í”„ë¡œì íŠ¸: ë©‹ì§„ ì‘ì‚¬ê°€ ë§Œë“¤ê¸°\n",
    "\n",
    "\n",
    "## Step 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "> https://aiffelstaticprd.blob.core.windows.net/media/documents/song_lyrics.zip  <br/>\n",
    "  song_lyrics ë°ì´í„° ë°›ê³  ì••ì¶•í’€ê¸°\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
    "\n",
    "- **`glob`** ëª¨ë“ˆ: íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ì‘ì—…ì„ í•˜ê¸°ê°€ ì•„ì£¼ ìš©ì´ \n",
    "- ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, raw_corpus ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " ['THE QUEEN _of_ HEARTS', '', '', '    The Queen of Hearts she made some tarts,', \"      All on a summer's day;\", '', '    The Knave of Hearts he stole those tarts,', '      And took them clean away.', '']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# ëª¨ë“  txt íŒŒì¼ì„ ëª¨ë‘ ì½ê³  raw_corpusì— ë¬¸ì¥ë‹¨ìœ„ë¡œ ì €ì¥.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:9])\n",
    "print(len(raw_corpus[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. ë°ì´í„° ì •ì œ\n",
    "\n",
    "- `preprocess_sentence()` í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ë°ì´í„°ë¥¼ ì •ì œ\n",
    "\n",
    "\n",
    "- ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë¬¸ì¥ì€ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì´ ê³¼ë„í•œ Paddingì„ ê°–ê²Œ í•˜ë¯€ë¡œ ì œê±° \n",
    "- ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë…¸ë˜ê°€ì‚¬ ì‘ì‚¬í•˜ê¸°ì— ì–´ìš¸ë¦¬ì§€ ì•Šì„ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ(ë¬¸ì¥ì„ í† í°í™” í–ˆì„ ë•Œ í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì„ í•™ìŠµë°ì´í„°ì—ì„œ ì œì™¸)\n",
    "\n",
    "\n",
    "\n",
    "* re ì°¸ê³ ìë£Œ: https://greeksharifa.github.io/%EC%A0%95%EA%B7%9C%ED%91%9C%ED%98%84%EC%8B%9D(re)/2018/07/20/regex-usage-01-basic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                  # ì •ê·œí‘œí˜„ì‹(ë¬¸ì¥ ë°ì´í„°ë¥¼ ì •ëˆí•˜ê¸° ìœ„í•´) \n",
    "import numpy as np         # ë³€í™˜ëœ ë¬¸ì¥ ë°ì´í„°(í–‰ë ¬)ì„ í¸í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´\n",
    "import tensorflow as tf  \n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()       # ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ì–‘ìª½ ê³µë°±ì„ ì‚­ì œ\n",
    "\n",
    "    # ì•„ë˜ 3ë‹¨ê³„ë¥¼ ê±°ì³ sentenceëŠ” ìŠ¤í˜ì´ìŠ¤ 1ê°œë¥¼ delimeterë¡œ í•˜ëŠ” ì†Œë¬¸ì ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜\n",
    "    # 1. íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)\n",
    "    # 2. ê³µë°± íŒ¨í„´ -> 'ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜'(ì§€ìš°ê² ë‹¤)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  \n",
    "    # 3. a-zA-Z?.!,Â¿ íŒ¨í„´ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì(ê³µë°±ë¬¸ìê¹Œì§€ë„) -> 'ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜'        \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip() # ê³µë°±ìœ¼ë¡œ ë°”ë€ë¶€ë¶„ ë‹¤ ì§€ìš°ê¸°\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'      \n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 175749\n",
      "Examples:\n",
      " ['<start> the queen of hearts <end>', '<start> the queen of hearts she made some tarts , <end>', '<start> all on a summer s day <end>']\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "\n",
    "\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(corpus))\n",
    "print(\"Examples:\\n\", corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "\n",
    "- `tokenize()` í•¨ìˆ˜ë¡œ ë°ì´í„°ë¥¼ Tensorë¡œ ë³€í™˜í•œ í›„, \n",
    "\n",
    "\n",
    "- `sklearn` ëª¨ë“ˆì˜ `train_test_split()` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬\n",
    "\n",
    "\n",
    "- **ë‹¨ì–´ì¥ì˜ í¬ê¸°ëŠ” 12,000 ì´ìƒ**ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”! \n",
    "\n",
    "\n",
    "- **ì´ ë°ì´í„°ì˜ 20%ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©**í•´ ì£¼ì„¸ìš”!\n",
    "\n",
    "> enc_train, enc_val, dec_train, dec_val = <ì½”ë“œ ì‘ì„±>\n",
    "\n",
    "ì—¬ê¸°ê¹Œì§€ ì˜¬ë°”ë¥´ê²Œ ì§„í–‰í–ˆì„ ê²½ìš°, ì•„ë˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    ">print(\"Source Train:\", enc_train.shape) <br/>\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "\n",
    "> out:\n",
    ">> Source Train: (124960, 14)\n",
    ">>Target Train: (124960, 14)\n",
    "\n",
    "---\n",
    "\n",
    "- ë§Œì•½ ê²°ê³¼ê°€ ë‹¤ë¥´ë‹¤ë©´ ì²œì²œíˆ ê³¼ì •ì„ ë‹¤ì‹œ ì‚´í´ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ë„ë¡ í•˜ì„¸ìš”! \n",
    "\n",
    "- ë§Œì•½ í•™ìŠµë°ì´í„° ê°¯ìˆ˜ê°€ 124960ë³´ë‹¤ í¬ë‹¤ë©´ ìœ„ Step 3.ì˜ ë°ì´í„° ì •ì œ ê³¼ì •ì„ ë‹¤ì‹œí•œë²ˆ ê²€í† "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   6 818 ...   0   0   0]\n",
      " [  2   6 818 ...   0   0   0]\n",
      " [  2  24  18 ...   0   0   0]\n",
      " ...\n",
      " [  2   5  90 ...   0   0   0]\n",
      " [  2   9 157 ...   0   0   0]\n",
      " [  2 160  15 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7fd2c3910ac8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175749, 347)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "\n",
    "    # <Tokenizer íŒ¨í‚¤ì§€ ìƒì„± by tensorflow>\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,  # ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜ \n",
    "        filters=' ',      # ë³„ë„ë¡œ ì „ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆì§€ë§Œ ì§€ê¸ˆì€ X\n",
    "        oov_token=\"<unk>\"  # ì‚¬ì „ì— ì—†ì—ˆë˜ ë‹¨ì–´ -> ì–´ë–¤ í† í°ìœ¼ë¡œ ëŒ€ì²´í• ì§€ ì„¤ì •\n",
    "    )\n",
    "    \n",
    "    # ë§Œë“¤ì–´ ë†“ì€ corpusë¡œë¶€í„° Tokenizerê°€ <ì‚¬ì „>ì„ ìë™ìƒì„±\n",
    "    tokenizer.fit_on_texts(corpus)   \n",
    "\n",
    "    # ì´í›„ tokenizerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥í•  <ë°ì´í„°ì…‹ì„ êµ¬ì¶•>\n",
    "    # tokenizerëŠ” êµ¬ì¶•í•œ ì‚¬ì „ìœ¼ë¡œë¶€í„° / corpusë¥¼ í•´ì„í•´ / Tensorë¡œ ë³€í™˜\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    \n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶”ê¸° ìœ„í•œ <padding ë©”ì†Œë“œ>ë¥¼ ì œê³µ\n",
    "    # maxlenì˜ ë””í´íŠ¸ê°’= None (corpusì˜ ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ê¸°ì¤€ìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ë§ì¶°ì§)\n",
    "    # ì¦‰, post: ë’·ìª½ì˜ ë‚¨ì€ ê³µê°„ì— 0ì´ ì¶”ê°€ë˜ì–´ ê¸¸ì´ê°€ ë§ì¶°ì§.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "27592\n"
     ]
    }
   ],
   "source": [
    "# <ìƒì„±ëœ ë‹¨ì–´ ì‚¬ì „ í™•ì¸>\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break\n",
    "        \n",
    "print(len(tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sentence:  (346,)\n",
      "Target Sentence:  (346,)\n"
     ]
    }
   ],
   "source": [
    "# tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ [ì†ŒìŠ¤ ë¬¸ì¥ = data]ì„ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ [íƒ€ê²Ÿ ë¬¸ì¥ = label]ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print('Source Sentence: ',src_input[0].shape) # data\n",
    "print('Target Sentence: ',tgt_input[0].shape) # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140599, 346)\n",
      "Target Train: (140599, 346)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val  = train_test_split(src_input, \n",
    "                                                            tgt_input, \n",
    "                                                            test_size=0.2,\n",
    "                                                            shuffle=True)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (175749, 346)\n",
      "Target Train: (175749, 346)\n"
     ]
    }
   ],
   "source": [
    "# source / target\n",
    "\n",
    "# tensorì—ì„œ ë§ˆì§€ë§‰ í† í° ìë¦„ -> [ì†ŒìŠ¤ ë¬¸ì¥]ì„ ìƒì„±\n",
    "# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ.\n",
    "enc_train = tensor[:, :-1]  \n",
    "\n",
    "# tensorì—ì„œ <start>ë¥¼ ìë¦„ -> [íƒ€ê²Ÿ ë¬¸ì¥]ì„ ìƒì„±\n",
    "dec_train = tensor[:, 1:]    \n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 346), (256, 346)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizerê°€ êµ¬ì¶•í•œ ë‹¨ì–´ì‚¬ì „ ë‚´ 7000ê°œì™€, ì—¬ê¸° í¬í•¨ë˜ì§€ ì•Šì€ 0:<pad>ë¥¼ í¬í•¨í•˜ì—¬ 7001ê°œ\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    \n",
    "\n",
    "# ğŸŒŸtf.data.Datasetê°ì²´ ìƒì„±\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°\n",
    "\n",
    "ëª¨ë¸ì˜ Embedding Sizeì™€ Hidden Sizeë¥¼ ì¡°ì ˆí•˜ë©° \n",
    "\n",
    "- 10 Epoch ì•ˆì— `val_loss` ê°’ì„ 2.2 ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•˜ì„¸ìš”! (LossëŠ” ì•„ë˜ ì œì‹œëœ Loss í•¨ìˆ˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©!)\n",
    "\n",
    "> #Loss  <br/>\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    ">generate_text(lyricist, tokenizer, init_sentence = \" `<start>` i love \", max_len=20)\n",
    "\n",
    "ê·¸ë¦¬ê³  ë©‹ì§„ ëª¨ë¸ì´ ìƒì„±í•œ ê°€ì‚¬ í•œ ì¤„ì„ ì œì¶œí•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2349\u001b[0m     \"\"\"\n\u001b[1;32m   2350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2351\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2352\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "model.fit(dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ ì¼ë‹¨ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ì‹¤ì œë¡œ ìƒì„±í• ë•ŒëŠ” ë£¨í”„ë¥¼ ëŒë©´ì„œ ë‹¨ì–´ í•˜ë‚˜ì”© ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤. \n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë°”ë¡œ ìƒˆë¡­ê²Œ ìƒì„±í•œ ë‹¨ì–´ê°€ ë©ë‹ˆë‹¤. \n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]  \n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ ìƒˆë¡­ê²Œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì…ë ¥ ë¬¸ì¥ì˜ ë’¤ì— ë¶™ì—¬ ì¤ë‹ˆë‹¤. \n",
    "        test_tensor = tf.concat([test_tensor, \n",
    "                                 tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # ìš°ë¦¬ ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´  \n",
    "        # while ë£¨í”„ë¥¼ ë˜ ëŒë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    # ìƒì„±ëœ tensor ì•ˆì— ìˆëŠ” word indexë¥¼ tokenizer.index_word ì‚¬ì „ì„ í†µí•´ \n",
    "    # ì‹¤ì œ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤. \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # ì´ê²ƒì´ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì´ ìƒì„±í•œ ìì—°ì–´ ë¬¸ì¥ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence = \" <start> i love \", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ˜­ ì´ë²ˆì£¼ì— ì´ì‚¬ ì¤€ë¹„ë¡œ ë„ˆë¬´ ë°”ë¹´ìŠµë‹ˆë‹¤ ã… ã…  \n",
    "# ğŸ˜­ğŸ˜­ì£¼ë§ê¹Œì§€ í•´ì„œ ë‹¤ì‹œ ì—…ë¡œë“œì‹œí‚¤ê² ìŠµë‹ˆë‹¤ ã… ã… "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ ë£¨ë¸Œë¦­ ]\n",
    "\n",
    "1. ê°€ì‚¬ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ê°€?  <br/>\n",
    "\tí…ìŠ¤íŠ¸ ì œë„ˆë ˆì´ì…˜ ê²°ê³¼ê°€ ê·¸ëŸ´ë“¯í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„±ë˜ëŠ”ê°€?\n",
    "    \n",
    "\n",
    "2. ë°ì´í„°ì˜ ì „ì²˜ë¦¬ì™€ ë°ì´í„°ì…‹ êµ¬ì„± ê³¼ì •ì´ ì²´ê³„ì ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆëŠ”ê°€?  <br/>\n",
    "\tíŠ¹ìˆ˜ë¬¸ì ì œê±°, í† í¬ë‚˜ì´ì € ìƒì„±, íŒ¨ë”©ì²˜ë¦¬ ë“±ì˜ ê³¼ì •ì´ ë¹ ì§ì—†ì´ ì§„í–‰ë˜ì—ˆëŠ”ê°€?\n",
    "    \n",
    "\n",
    "3. í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì´ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆëŠ”ê°€?  <br/>\n",
    "\tí…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì˜ validation lossê°€ 2.2 ì´í•˜ë¡œ ë‚®ì•„ì¡ŒëŠ”ê°€?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ íšŒ ê³  ]\n",
    "\n",
    "1. ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ ì–´ë ¤ì› ë˜ ì : \n",
    "\n",
    "\n",
    "2. í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ì•Œì•„ë‚¸ ì  í˜¹ì€ ì•„ì§ ëª¨í˜¸í•œ ì .\n",
    "\n",
    "\n",
    "- ì•Œì•„ë‚¸ ì : \n",
    "\n",
    "\n",
    "- ëª¨í˜¸í•œ ì : \n",
    "\n",
    "\n",
    "3. ë£¨ë¸Œë¦­ í‰ê°€ ì§€í‘œë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì‹œë„í•œ ê²ƒë“¤.\n",
    "  - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
