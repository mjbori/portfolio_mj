{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ 개 요 ]\n",
    "\n",
    "- Dense: Fully connected layer\n",
    "\n",
    "- Dense는 [ output = activation(dot(input, kernel) + bias) ] 인 연산을 구현\n",
    "    - activation: 활성화 요소로 전달된 활성화 함수\n",
    "    - kernel: 레이어에 의해 생성된 가중치 행렬\n",
    "    - bias: 레이어에 의해 생성된 bias벡터\n",
    "    \n",
    "- [Note] \n",
    "    - 레이어(계층)로 들어가는 입력이 2보다 큰 rank(차원)를 갖는다면 dense는 입력의 마지막 축과 커널 축을 따라 입력과 kernel(가중치) 사이에 dot product를 계산한다(tf.tensordot사용).\n",
    "    - (ex) input이 (batch_size, d0, d1)라면, \n",
    "      (d1, units)형태의 텐서를 생성하고 \n",
    "      모든 하위 텐서는 (1, 1, d1) 형태로 입력의 축 2를 따라 작동하여 \n",
    "      output은 (batch_size, d0, 단위)의 형태가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create a `Sequential` model and add a Dense layer as the first layer.  \n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.Input(shape=(16,)))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "# Now the model will take as input arrays of shape (None, 16)  \n",
    "# and output arrays of shape (None, 32).  \n",
    "# Note that after the first layer, you don't need to specify  \n",
    "# the size of the input anymore:  \n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.output_shape\n",
    "(None, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [ Arguments ]\n",
    "\n",
    "1. **units**: Positive integer, dimensionality of the output space.\n",
    "    - 양의 정수, 출력 공간의 차수\n",
    "\n",
    "2. **activation**: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    - 사용할 활성화 함수, 아무 것도 지정하지 않으면 활성화가 적용되지 않음(예: \"선형\" 활성화: a(x) = x)\n",
    "    - 옵션(문자열)으로 넣을 수도 있고, 함수로 넣을 수도 있고, 클래스로 넣을 수도 있음\n",
    "    - 클래스로 넣으면 쪼개서 넣기가 가능\n",
    "    - 아래 4개의 dense는 모두 같은것\n",
    "    - Dense(16, activation='relu')\n",
    "    - Dense(16, activation=tf.keras.activations.relu)\n",
    "    - Dense(16, activation=tf.keras.layers.Activation(activation='relu'))\n",
    "    - Dense(16, activation=tf.keras.layers.ReLU())\n",
    "    - https://keras.io/api/layers/activations/\n",
    "\n",
    "3. **use_bias**: Boolean, whether the layer uses a bias vector.\n",
    "    - 계층이 바이어스 벡터를 사용하는지 여부를 나타내는 부울 parameter\n",
    "    - default = True\n",
    "    \n",
    "---\n",
    "\n",
    "4. **kernel_initializer**: Initializer for the kernel weights matrix.\n",
    "    - 초기 가중치 설정 매개변수\n",
    "    - model.add(Dense(64,\n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros'))\n",
    "    - random_uniform : -0.05 ~ 0.05로 임의의 값으로 초기화\n",
    "    - random_normal : 평균이 0, 표준편차가 0.05로 정규분포에 따라 초기화\n",
    "    - zeros : 모든 가중치를 0으로 초기화 \n",
    "    - he_normal : He 정규분포 초기값 설정 (ResNet의 창시자Kaiming He가 제안)\n",
    "\n",
    "5. **bias_initializer**: Initializer for the bias vector.\n",
    "    - bias vector를 위한 초기화 함수\n",
    "    \n",
    "---\n",
    "\n",
    "6. **kernel_regularizer**: Regularizer function applied to the kernel weights matrix.\n",
    "    - 커널 가중치 행렬에 적용되는 정규화 함수\n",
    "    - https://keras.io/api/layers/regularizers/\n",
    "\n",
    "7. **bias_regularizer**: Regularizer function applied to the bias vector.\n",
    "    - Regularizer 함수가 바이어스 벡터에 적용\n",
    "    - https://keras.io/api/layers/regularizers/\n",
    "\n",
    "8. **activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\").\n",
    "    - 출력값을 정규화하는 방법에 대한 파라미터\n",
    "    - https://keras.io/api/layers/regularizers/\n",
    "    \n",
    "---\n",
    "어느정도 값이 넘어가면 짤라라! \n",
    "\n",
    "9. **kernel_constraint**: Constraint function applied to the kernel weights matrix.\n",
    "    - kernel weight matrix에 적용된 제약 조건 함수\n",
    "    - https://keras.io/api/layers/constraints/\n",
    "    - 다양한 정규화 함수나 규제를 통해서 결과를 개선\n",
    "10. **bias_constraint**: Constraint function applied to the bias vector.\n",
    "    - bias vector에 적용되는 제한함수.\n",
    "    - https://keras.io/api/layers/constraints/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [ Vs Pytorch ]\n",
    "\n",
    "- https://discuss.pytorch.org/t/pytorch-equivalent-of-keras/29412\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
